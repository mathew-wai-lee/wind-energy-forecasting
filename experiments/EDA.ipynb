{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Wind Turbine Anatomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wind Turbine Aerodynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pip Install, Init, and Start Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql import types as T\n",
    "# from pyspark.sql.functions import broadcast\n",
    "# from pyspark.sql.functions import col\n",
    "# from pyspark.sql.functions import *\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error, f1_score, mean_absolute_percentage_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.tools import diff\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.cwd().joinpath(\"mlruns\").as_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of cores\n",
    "# num_cores = sc.defaultParallelism\n",
    "\n",
    "# Get the number of executors\n",
    "# num_executors = sc.getConf().get(\"spark.executor.instances\")\n",
    "\n",
    "# print(num_cores)\n",
    "# print(num_executors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../src/main/resources/data/Turbine_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = (spark.read\n",
    "#       .option(\"header\", \"true\")\n",
    "#       .option(\"inferSchema\", \"true\")\n",
    "#       .csv(\"../data/Turbine_Data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_renamed = df.withColumnRenamed(\"_c0\", \"datetime_stamp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed = df.rename({\"Unnamed: 0\": \"datetime_stamp\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed['datetime_stamp'] = pd.to_datetime(df_renamed['datetime_stamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed_sorted = df_renamed.set_index(\"datetime_stamp\").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed_sorted.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df_indexed_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_casted = df_renamed.withColumn(\"datetime_stamp\", col(\"datetime_stamp\").cast(\"timestamp\")).drop(col(\"_c0\"))\n",
    "# df_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtered = df_casted.where(\n",
    "#     to_date(col(\"datetime_stamp\")) ==\"2018-01-01\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = df_casted.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile = ProfileReport(pdf, title=\"YData Profiling Report\")\n",
    "# profile.to_file('profiling_report.html');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Empty or Single Value Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_drop_bad_cols = pdf.drop(columns=['ControlBoxTemperature', 'TurbineStatus', 'WTG'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pdf_drop_bad_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Highly Correlated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = pdf_drop_bad_cols.corr()\n",
    "corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix.abs().unstack()[\"NacellePosition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix.abs().unstack().sort_values(ascending=False).drop_duplicates().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop highly correlated features\n",
    "### pdf[[\"WindSpeed\",\"ActivePower\"]]\n",
    "### pdf[[\"RotorRPM\", \"GeneratorRPM\"]]\n",
    "### pdf[[\"AmbientTemperatue\", \"MainBoxTemperature\"]]\n",
    "### pdf[[\"Blade1PitchAngle\",\"Blade2PitchAngle\", \"Blade3PitchAngle\"]]\n",
    "### pdf[[\"NacellePosition\",\"WindDirection\"]]\n",
    "\n",
    "\n",
    "df_drop_corr_cols = pdf.drop([\"NacellePosition\",\"RotorRPM\", \"AmbientTemperatue\", \"Blade2PitchAngle\", \"Blade3PitchAngle\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_corr_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Null Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_nulls = df_drop_corr_cols.dropna(subset=[\"ActivePower\"]).sort_index()\n",
    "# df_drop_nulls = df_drop_corr_cols.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Percent non-null\n",
    "df_drop_nulls.shape[0]/pdf.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_nulls.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_by_date(df: pd.DataFrame, train_test_date_split_mid_pt: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    X_train = df.iloc[df.index <= train_test_date_split_mid_pt]\n",
    "    X_test = df.iloc[df.index > train_test_date_split_mid_pt]\n",
    "    # print(X_train)\n",
    "    # print(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "train_test_date_split_mid_pt = \"2019-03-01\"\n",
    "\n",
    "X_train_time_series, X_test_time_series = train_test_split_by_date(df_drop_nulls, train_test_date_split_mid_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot x_train and x_test\n",
    "y_var = \"ActivePower\"\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_train_time_series[y_var])\n",
    "plt.plot(X_test_time_series[y_var])\n",
    "plt.title(\"Active Power Generated\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"ActivePower\")\n",
    "time_series_fn = f\"./figures/time_series.png\"\n",
    "plt.savefig(time_series_fn)\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_mean(X_train: pd.DataFrame, X_test: pd.DataFrame, y_variable: str, grain: str) -> tuple[pd.Series, pd.Series]:\n",
    "    return X_train[y_variable].resample(grain).mean(), X_test[y_variable].resample(grain).mean()\n",
    "\n",
    "X_train_resampled, X_test_resampled = resample_mean(X_train_time_series, X_test_time_series, y_variable=y_var, grain=\"W-MON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot x_train and x_test\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_train_resampled)\n",
    "plt.plot(X_test_resampled)\n",
    "plt.title(\"Active Power Generated\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"ActivePower\")\n",
    "time_series_resampled_fn = f\"./figures/time_series_resampled.png\"\n",
    "plt.savefig(time_series_resampled_fn)\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _apply_differencing(self, time_series: pd.Series):\n",
    "    # return diff(time_series, k_seasonal_diff=1, seasonal_periods=self.seasonal_periods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Forecast (SARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.data\n",
    "\n",
    "mlflow.set_experiment(\"wind-energy-forecasting\")\n",
    "\n",
    "# Enable autologging for scikit-learn\n",
    "mlflow.statsmodels.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(self, time_series: pd.Series) -> None:\n",
    "    result = adfuller(time_series, autolag='AIC')\n",
    "    p_value = result[1]\n",
    "    print(f'ADF Statistic: {result[0]}')\n",
    "    print(f'p-value: {p_value}')\n",
    "    print('Stationary' if p_value < 0.05 else 'Non-Stationary')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_forecast(X_test: pd.Series, forecast) -> tuple[float, float, float, float]:\n",
    "    # print(X_test[1:].sort_index())\n",
    "    # print(forecast.sort_index())\n",
    "\n",
    "    mae = mean_absolute_error(X_test[1:], forecast)\n",
    "    mape = mean_absolute_percentage_error(X_test[1:], forecast)\n",
    "    mse = mean_squared_error(X_test[1:], forecast)\n",
    "    rmse = root_mean_squared_error(X_test[1:], forecast)\n",
    "\n",
    "    print(f'MAE: {mae}')\n",
    "    print(f'MAE: {mape}')\n",
    "    print(f'MSE: {mse}')\n",
    "    print(f'RMSE: {rmse}')\n",
    "\n",
    "    return mae, mape, mse, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Setting nested=True will create a child run under the parent run.\n",
    "    with mlflow.start_run(nested=True, run_name=f\"trial_{trial.number}\") as child_run:\n",
    "        max_p = trial.suggest_int(\"max_p\", 1, 3)\n",
    "        max_d = trial.suggest_int(\"max_d\", 1, 3)\n",
    "        max_q = trial.suggest_int(\"max_q\", 1, 3)\n",
    "        max_P = trial.suggest_int(\"max_P\", 1, 3)\n",
    "        max_D = trial.suggest_int(\"max_D\", 1, 3)\n",
    "        max_Q = trial.suggest_int(\"max_Q\", 1, 3)\n",
    "        seasonal_periods = 26\n",
    "        forecast_n_steps = len(X_test_resampled) - 1\n",
    "\n",
    "        X_train_dataset = mlflow.data.from_pandas(X_train_time_series)\n",
    "        X_test_dataset = mlflow.data.from_pandas(X_test_time_series)\n",
    "        X_train_resampled_dataset = mlflow.data.from_pandas(pd.DataFrame(X_train_resampled))\n",
    "        X_test_resampled_dataset = mlflow.data.from_pandas(pd.DataFrame(X_test_time_series))\n",
    "\n",
    "        params = {\n",
    "            \"max_p\": max_p,\n",
    "            \"max_d\": max_d,\n",
    "            \"max_q\": max_q,\n",
    "            \"max_P\": max_P,\n",
    "            \"max_D\": max_D,\n",
    "            \"max_Q\": max_Q,\n",
    "            \"seasonal_periods\": seasonal_periods,\n",
    "            \"random_state\": 42,\n",
    "            \n",
    "        }\n",
    "\n",
    "        # Log current trial's parameters\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Optional: Set a tag that we can use to remind ourselves what this run was for\n",
    "        mlflow.set_tag(\"Training Info\", \"Time Series forecasting via SARIMA\")\n",
    "\n",
    "        mlflow.log_inputs(\n",
    "            datasets=[X_train_dataset, X_test_dataset, X_train_resampled_dataset, X_test_resampled_dataset],\n",
    "        contexts=[\"Training\", \"Testing\", \"Training Resampled\", \"Testing Resampled\"],\n",
    "                tags_list=[None, {\"my_tag\": \"tag_value\"}, None, None],\n",
    "        )\n",
    "\n",
    "\n",
    "        ### Log ACF\n",
    "        plot_acf(X_train_resampled)\n",
    "        acf_fn = f\"./figures/acf_{trial.number}.png\"\n",
    "        plt.savefig(acf_fn)\n",
    "        mlflow.log_artifact(acf_fn) # logging to mlflow      \n",
    "        plt.close()\n",
    "        # plt.show()\n",
    "        \n",
    "        \n",
    "        ### Log PACF\n",
    "        plot_pacf(X_train_resampled)\n",
    "        pacf_fn = f\"./figures/pacf_{trial.number}.png\"\n",
    "        plt.savefig(pacf_fn)\n",
    "        mlflow.log_artifact(pacf_fn) # logging to mlflow      \n",
    "        plt.close()\n",
    "        # plt.show()\n",
    "                \n",
    "        ## Fit and forecast\n",
    "        model = SARIMAX(X_train_resampled, order=(max_p, max_d, max_q), seasonal_order=(max_P, max_D, max_Q, seasonal_periods))\n",
    "        fit_model = model.fit()\n",
    "        forecast = fit_model.get_forecast(steps=forecast_n_steps)\n",
    "        forecast_values = forecast.predicted_mean\n",
    "        forecast_ci = forecast.conf_int()\n",
    "        \n",
    "\n",
    "        ### Plot Training, Testing, Forecast, and CI\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(X_train_resampled, label='Training')\n",
    "        plt.plot(X_test_resampled, label='Actuals', color='orange')\n",
    "        plt.plot(forecast_values, label='Forecast', color='red')\n",
    "        plt.fill_between(forecast_ci.index,\n",
    "                        forecast_ci.iloc[:,0],\n",
    "                        forecast_ci.iloc[:,1],\n",
    "                        color='k', alpha=.15)\n",
    "        plt.title(\"Active Power Forecast\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Sales\")\n",
    "        plt.legend()\n",
    "        train_test_fcst_ci_fn = f\"./train_test_fcst_ci_{trial.number}.png\"\n",
    "        plt.savefig(train_test_fcst_ci_fn)\n",
    "        mlflow.log_artifact(train_test_fcst_ci_fn) # logging to mlflow      \n",
    "        plt.close\n",
    "        # plt.show()\n",
    "\n",
    "        # ## Log plot_diagnostics()\n",
    "        # fit_model.plot_diagnostics()\n",
    "        # diagnostics_fn = f\"./figures/diagnostics_{trial.number}.png\"\n",
    "        # plt.savefig(diagnostics_fn)\n",
    "        # mlflow.log_artifact(diagnostics_fn) # logging to mlflow      \n",
    "        # plt.close()\n",
    "        # # plt.show()\n",
    "        \n",
    "        # Predict on the test set, compute and log the loss metric\n",
    "        mae, mape, mse, rmse = evaluate_forecast(X_test_resampled, forecast_values)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"mape\", mape)\n",
    "        mlflow.log_metric(\"mse\", mse)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        # mlflow.log_metric(\"residuals\", residuals)\n",
    "        mlflow.log_metrics({\"mae\": mae, \"mape\": mape, \"mse\": mse, \"rmse\": rmse})\n",
    "\n",
    "        # Log the model file\n",
    "        # mlflow.statsmodels.log_model(fit_model, name=\"model\", remove_data=True)\n",
    "\n",
    "        # Make it easy to retrieve the best-performing child run later\n",
    "        trial.set_user_attr(\"run_id\", child_run.info.run_id)\n",
    "        return fit_model.aic\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an MLflow run)\n",
    "with mlflow.start_run(run_name='SARIMAX') as run:\n",
    "    n_trials = 30\n",
    "    mlflow.log_param(\"n_trials\", n_trials)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    # study.optimize(objective, callbacks=[optuna.study.MaxTrialsCallback(n_trials, states=(optuna.trial.TrialState.COMPLETE,))])\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Log the best trial and its run ID\n",
    "    mlflow.log_params(study.best_trial.params)\n",
    "    mlflow.log_metrics({\"best_error\": study.best_value})\n",
    "    if best_run_id := study.best_trial.user_attrs.get(\"run_id\"):\n",
    "        mlflow.log_param(\"best_child_run_id\", best_run_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: add pacf and acf for resampled\n",
    "### TODO: Add next forecasating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/machine-learning/sarima-seasonal-autoregressive-integrated-moving-average/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply Differencecing\n",
    "# print(\"Apply Differencing\")\n",
    "# X_train_resampled_differenced = pd.Series(self._apply_differencing(X_train_resampled))\n",
    "# X_test_resampled_differenced = pd.Series(self._apply_differencing(X_test_resampled))\n",
    "\n",
    "### Plot X_train with differencing and resampling\n",
    "# print(\"Plotting X_train with Differencing and resampling\")\n",
    "# weekly_forecast._plot_time_series(X_train_resampled, X_test_resampled)\n",
    "# weekly_forecast._check_stationarity(X_train_resampled)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wind-energy-forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
