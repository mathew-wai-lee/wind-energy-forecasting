{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import broadcast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#             .config(\"spark.driver.memory\", \"6g\") \\\n",
    "#             .config(\"spark.sql.adaptive.enabled\", \"True\")\\\n",
    "#             .master(\"local\").getOrCreate();\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the number of cores\n",
    "num_cores = sc.defaultParallelism\n",
    "\n",
    "# Get the number of executors\n",
    "num_executors = sc.getConf().get(\"spark.executor.instances\")\n",
    "\n",
    "print(num_cores)\n",
    "print(num_executors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Basic rdd example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of the first 100 whole numbers\n",
    "rdd = sc.parallelize(range(100 + 1))\n",
    "rdd.sum()\n",
    "# 5050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Join Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames\n",
    "data1 = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
    "data2 = [(\"Alice\", \"Engineer\"), (\"Bob\", \"Doctor\"), (\"David\", \"Lawyer\")]\n",
    "df1 = spark.createDataFrame(data1, [\"Name\", \"ID\"])\n",
    "df2 = spark.createDataFrame(data2, [\"Name\", \"Profession\"])\n",
    "\n",
    "# Join DataFrames\n",
    "joined_df = df1.join(df2, \"Name\")\n",
    "\n",
    "# Show the result\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A broadcast join is useful when one DataFrame (typically smaller) can fit entirely into \n",
    "# memory on each executor node, allowing it to be efficiently broadcasted to all executor nodes for join operations. Here's an example using a broadcast join in PySpark:\n",
    "\n",
    "# Two DataFrames (employees_df and departments_df) are created from dummy data.\n",
    "# We perform a broadcast join using the join method on the employees_df. The broadcast function is applied to the departments_df DataFrame, indicating that it should be broadcasted to all executor nodes.\n",
    "# The join operation is performed based on the \"dept_id\" column, and the result is stored in the DataFrame broadcast_df.\n",
    "# Finally, we show the result of the join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data for employees and their departments\n",
    "employees_data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 1), (\"David\", 3)]\n",
    "departments_data = [(1, \"HR\"), (2, \"Engineering\"), (3, \"Finance\")]\n",
    "\n",
    "# Create DataFrames from the dummy data\n",
    "employees_df = spark.createDataFrame(employees_data, [\"name\", \"dept_id\"])\n",
    "departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\"])\n",
    "\n",
    "# Perform broadcast join\n",
    "broadcast_join_df = employees_df.join(broadcast(departments_df),\n",
    "    on=\"dept_id\",\n",
    "    how=\"inner\")\n",
    "\n",
    "# Show the result\n",
    "broadcast_join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a shuffle hash join, both DataFrames are partitioned based on the join key, \n",
    "# and then shuffled across the network so that rows with the same join key end up in the same partition. This allows Spark to efficiently join the corresponding rows from both DataFrames.\n",
    "\n",
    "# Two DataFrames (employees_df and departments_df) are created from dummy data.\n",
    "# We perform a shuffle hash join using the join method on the employees_df. The join operation is performed based on the \"dept_id\" column, and we specify how=\"inner\" to perform an inner join.\n",
    "# Spark partitions both DataFrames based on the join key (\"dept_id\") and shuffles the data across the network so that rows with the same join key end up in the same partition.\n",
    "# Spark then efficiently joins the corresponding rows from both DataFrames based on the join key.\n",
    "# Finally, we show the result of the join operation.\n",
    "\n",
    "# If a broadcast hash join can be used (by the broadcast hint or by total size of a relation), Spark SQL chooses it over other joins (see JoinSelection execution planning strategy).\n",
    "\n",
    "# Shuffle Hash Join:\n",
    "\n",
    "# Size of Datasets: Shuffle hash joins are typically more efficient when one or both of the datasets being joined are small enough to fit entirely in memory on each executor node. If one dataset is small enough to be broadcasted, a shuffle hash join can be more efficient.\n",
    "# Data Distribution: Shuffle hash joins work well when the data is evenly distributed across partitions and the join keys have high cardinality. In this case, the shuffle operation redistributes the data across partitions based on the hash of the join key, which can lead to more balanced partition sizes.\n",
    "# Memory Usage: Shuffle hash joins are less memory-intensive compared to sort merge joins, as they don't require sorting of data. This can be advantageous when memory resources are limited.\n",
    "# Sort Merge Join:\n",
    "\n",
    "# Size of Datasets: Sort merge joins are generally more suitable for joining large datasets that cannot fit entirely in memory on each executor node. They involve sorting the data, which can be memory-intensive, but they can handle larger datasets more efficiently than shuffle hash joins in many cases.\n",
    "# Data Distribution: Sort merge joins can handle skewed data distributions and low cardinality join keys more effectively than shuffle hash joins. They don't rely on hashing and shuffling data across partitions, so they can handle data skewness without causing performance issues.\n",
    "# Memory Usage: Sort merge joins are more memory-intensive compared to shuffle hash joins because they involve sorting large datasets. They may require more memory resources, so it's important to ensure that sufficient memory is available on the cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"false\")\n",
    "\n",
    "# Dummy data for employees and their departments\n",
    "employees_data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 1), (\"David\", 3)]\n",
    "departments_data = [(1, \"HR\"), (2, \"Engineering\"), (3, \"Finance\")]\n",
    "\n",
    "# Create DataFrames from the dummy data\n",
    "employees_df = spark.createDataFrame(employees_data, [\"name\", \"dept_id\"])\n",
    "departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\"])\n",
    "\n",
    "# Repartition DataFrames based on the join key\n",
    "employees_repartitioned = employees_df.repartition(\"dept_id\")\n",
    "departments_repartitioned = departments_df.repartition(\"dept_id\")\n",
    "\n",
    "# Perform shuffle hash join\n",
    "shuffle_hash_join_df = employees_repartitioned.join(\n",
    "    departments_repartitioned.hint(\"SHUFFLE_HASH\"),\n",
    "    on=\"dept_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "shuffle_hash_join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sort merge join is typically used when joining large datasets that cannot fit into memory on each executor node. \n",
    "### It efficiently handles large datasets by sorting them and then merging them together based on the join key.\n",
    "\n",
    "# Two DataFrames (employees_df and departments_df) are created from dummy data.\n",
    "# We perform a sort merge join using the join method on the employees_df. The join operation is performed based on the \"dept_id\" column, and we specify how=\"inner\" to perform an inner join.\n",
    "# Spark sorts both DataFrames based on the join key (\"dept_id\") and then merges them together.\n",
    "# Finally, we show the result of the join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")\n",
    "\n",
    "# Dummy data for employees and their departments\n",
    "employees_data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 1), (\"David\", 3)]\n",
    "departments_data = [(1, \"HR\"), (2, \"Engineering\"), (3, \"Finance\")]\n",
    "\n",
    "# Create DataFrames from the dummy data\n",
    "employees_df = spark.createDataFrame(employees_data, [\"name\", \"dept_id\"])\n",
    "departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\"])\n",
    "\n",
    "# Perform sort merge join\n",
    "sort_merge_join_df = employees_df.join(\n",
    "    departments_df,\n",
    "    on=\"dept_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "sort_merge_join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo - The importance of partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy graph data representing nodes and their outgoing links\n",
    "links = spark.sparkContext.parallelize([\n",
    "    (\"A\", [\"B\", \"C\"]),\n",
    "    (\"B\", [\"C\"]),\n",
    "    (\"C\", [\"A\"]),\n",
    "    (\"D\", [\"A\", \"C\"])\n",
    "]).partitionBy(4)  # Partition the data into 4 partitions\n",
    "\n",
    "# Initialize ranks for each node\n",
    "ranks = links.map(lambda pair: (pair[0], 1.0))\n",
    "\n",
    "# Perform 10 iterations of PageRank\n",
    "for _ in range(10):\n",
    "    # Compute contributions of each node to its neighbors\n",
    "    contributions = links.join(ranks).flatMap(lambda pair: [(dest, pair[1][1] / len(pair[1][0])) for dest in pair[1][0]])\n",
    "\n",
    "    # Aggregate contributions to compute new ranks\n",
    "    ranks = contributions.reduceByKey(lambda a, b: a + b).mapValues(lambda rank: 0.15 + 0.85 * rank)\n",
    "\n",
    "# Collect the final ranks\n",
    "final_ranks = ranks.collect()\n",
    "print(final_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data with 20 records and a partitionable column\n",
    "data = [(i, \"Name_\" + str(i), str(i % 5)) for i in range(1, 101)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"ID\", \"Name\", \"Department\"])\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "df = df.repartition(20)\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data with 20 records and a partitionable column\n",
    "data = [(i, \"Name_\" + str(i), str(i % 5)) for i in range(1, 101)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"ID\", \"Name\", \"Department\"])\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "df = df.repartition(1)\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data with 20 records and a partitionable column\n",
    "data = [(i, \"Name_\" + str(i), str(i % 5)) for i in range(1, 101)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"ID\", \"Name\", \"Department\"])\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "df = df.repartition(1000)\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coalesce and Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The output will show that the number of partitions is reduced from 4 to 2 after applying coalesce(). \n",
    "### Note that coalesce() is a narrow transformation, meaning it does not trigger a full shuffle of the data, but it can still cause data movement between partitions. It's typically used to optimize the number of partitions for better parallelism and resource utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data with 20 records and a partitionable column\n",
    "data = [(i, \"Name_\" + str(i), str(i % 5)) for i in range(1, 101)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"ID\", \"Name\", \"Department\"])\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "df = df.coalesce(1)\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition() triggers a full shuffle of the data across the cluster, \n",
    "# so it's more expensive compared to coalesce(). It's typically used when you want to either increase or decrease the number of partitions to a specific value, regardless of the current number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data with 20 records and a partitionable column\n",
    "data = [(i, \"Name_\" + str(i), str(i % 5)) for i in range(1, 101)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"ID\", \"Name\", \"Department\"])\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "df = df.repartition(1)\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use repartition() when you want to explicitly change the number of partitions to a specific value, regardless of the current number of partitions.\n",
    "# It triggers a full shuffle of the data across the cluster, which can be computationally expensive.\n",
    "# It's useful when you need to increase or decrease the number of partitions to distribute the data more evenly or to prepare for operations that benefit from a specific number of partitions (e.g., join operations).\n",
    "# Use repartition() when you want to increase the number of partitions, for example, to prepare for parallel operations that require more parallelism.\n",
    "\n",
    "# Use coalesce() when you want to reduce the number of partitions, ideally to a smaller number than the current number of partitions.\n",
    "# It's a narrow transformation that minimizes data movement and avoids a full shuffle whenever possible.\n",
    "# It's useful when you want to optimize resource utilization and reduce the overhead of managing many partitions, without triggering a full shuffle.\n",
    "# Use coalesce() when you want to decrease the number of partitions, for example, to reduce memory overhead or to optimize performance when the current number of partitions is higher than necessary.\n",
    "\n",
    "\n",
    "# coalesce may run faster than repartition, but unequal sized partitions are generally slower to work with than equal sized partitions. You'll usually need to repartition datasets after filtering a large data set. \n",
    "# I've found repartition to be faster overall because Spark is built to work with equal sized partitions.\n",
    "\n",
    "# If you need to filter your data before writing, then repartition is much more suitable than coalesce, since coalesce will be pushed-down right before the loading operation.\n",
    "\n",
    "# For instance: load().map(…).filter(…).coalesce(1).save()\n",
    "\n",
    "# translates to: load().coalesce(1).map(…).filter(…).save()\n",
    "\n",
    "# This means that all your data will collapse into a single partition, where it will be filtered, losing all parallelism. This happens even for very simple filters like column='value'.\n",
    "\n",
    "# This does not happen with repartition: load().map(…).filter(…).repartition(1).save()\n",
    "\n",
    "# In such case, filtering happens in parallel on the original partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29), (\"David\", 55)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Cache the DataFrame\n",
    "df.cache()\n",
    "\n",
    "# Perform some transformations and actions\n",
    "filtered_df = df.filter(df[\"Age\"] > 30)\n",
    "count1 = filtered_df.count()\n",
    "\n",
    "# Perform more transformations and actions\n",
    "selected_df = df.select(\"Name\")\n",
    "count2 = selected_df.count()\n",
    "\n",
    "# Show the counts\n",
    "print(\"Count 1:\", count1)\n",
    "print(\"Count 2:\", count2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://w.amazon.com/bin/view/BDT/Products/Cradle/Docs/Coalescence/\n",
    "https://mungingdata.com/apache-spark/filter-where/\n",
    "https://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce\n",
    "https://medium.com/airbnb-engineering/on-spark-hive-and-small-files-an-in-depth-look-at-spark-partitioning-strategies-a9a364f908"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
